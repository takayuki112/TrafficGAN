{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takayuki/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from gangen.cdcgan import Generator, Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/takayuki/Desktop/sem6/DL/mini_proj_TRAFFIC/data/'\n",
    "train_dir = os.path.join(data_dir, 'preprocessed', 'Train')\n",
    "\n",
    "data_tf = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.08, 0.08), scale=(0.9, 1.1), shear=5),\n",
    "    transforms.Resize((32, 32)),\n",
    "    \n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "   \n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))], p=0.3),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.01, 0.04), ratio=(0.5, 2.0), value=0), # value=0 for black box\n",
    "\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root = train_dir, transform=data_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "noise_dim = 100\n",
    "n_classes = 43\n",
    "embedding_dim = 64\n",
    "lambda_gp = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting class distribution...\n",
      "Class counts: [ 210. 2220. 2010. 1320. 2100. 2160.  780.  630.  420. 1110. 1200.  210.\n",
      " 2250.  360.  330.  390.  510.  270. 1500.  600.  240.  540.  270. 1410.\n",
      "  450.  780.  240.  689.  420. 1200.  390.  210. 2070.  300. 1980.  360.\n",
      "  240.  240. 1860.  420. 1440. 1410. 1470.]\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(\n",
    "    full_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Counting class distribution...\")\n",
    "class_counts = np.zeros(n_classes)\n",
    "for _, label in full_dataset:\n",
    "    class_counts[label] += 1\n",
    "    \n",
    "print(\"Class counts:\", class_counts)\n",
    "\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / np.sum(class_weights) * n_classes  # Normalize\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(noise_dim=noise_dim, n_classes=n_classes, embedding_dim=embedding_dim).to(device)\n",
    "D = Discriminator(n_classes=n_classes, embedding_dim=embedding_dim).to(device)\n",
    "\n",
    "# Set up optimizers\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
    "\n",
    "# Loss function (BCE with logits for stability)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard\n",
    "log_dir = 'logs/gan_training_' + time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|          | 0/612 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Sequential.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 93\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Use weighted sampling for fake labels to address class imbalance\u001b[39;00m\n\u001b[1;32m     91\u001b[0m fake_labels_dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(class_weights, batch_size, replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 93\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m \u001b[43mG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_labels_dist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Classify fake batch with D\u001b[39;00m\n\u001b[1;32m     96\u001b[0m d_fake_output \u001b[38;5;241m=\u001b[39m D(fake_images\u001b[38;5;241m.\u001b[39mdetach(), fake_labels_dist)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/sem6/DL/mini_proj_TRAFFIC/TrafficGAN/gangen/cdcgan.py:258\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, z, y)\u001b[0m\n\u001b[1;32m    254\u001b[0m z_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([z, y_emb], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    256\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial(z_y)\n\u001b[0;32m--> 258\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample2(x, y)\n\u001b[1;32m    260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample3(x, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Sequential.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "def generate_samples(num_samples=16, fixed_noise=None, fixed_labels=None):\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        if fixed_noise is None:\n",
    "            fixed_noise = torch.randn(num_samples, noise_dim, device=device)\n",
    "        if fixed_labels is None:\n",
    "            # Generate samples across different classes\n",
    "            fixed_labels = torch.arange(0, min(n_classes, num_samples), device=device)\n",
    "            # Repeat labels if num_samples > n_classes\n",
    "            fixed_labels = fixed_labels.repeat(num_samples // min(n_classes, num_samples) + 1)[:num_samples]\n",
    "        \n",
    "        fake_samples = G(fixed_noise, fixed_labels)\n",
    "        \n",
    "    # Convert to displayable format\n",
    "    fake_samples = fake_samples.detach().cpu().numpy()\n",
    "    # Move channel dimension to the end for plotting\n",
    "    fake_samples = np.transpose(fake_samples, (0, 2, 3, 1))\n",
    "    # Denormalize\n",
    "    fake_samples = fake_samples * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    fake_samples = np.clip(fake_samples, 0, 1)\n",
    "    \n",
    "    return fake_samples, fixed_labels.cpu().numpy()\n",
    "\n",
    "# Create a grid of images\n",
    "def create_image_grid(images, labels, nrow=4):\n",
    "    ncol = images.shape[0] // nrow\n",
    "    fig, axes = plt.subplots(nrow, ncol, figsize=(12, 12))\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < images.shape[0]:\n",
    "            ax.imshow(images[i])\n",
    "            ax.set_title(f\"Class: {labels[i]}\")\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Function to save checkpoint\n",
    "def save_checkpoint(G, D, optimizer_G, optimizer_D, epoch, filename='checkpoint.pth'):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': G.state_dict(),\n",
    "        'discriminator_state_dict': D.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "    }, filename)\n",
    "\n",
    "# Create fixed noise for visualization\n",
    "fixed_noise = torch.randn(16, noise_dim, device=device)\n",
    "fixed_labels = torch.arange(0, min(n_classes, 16), device=device).repeat(16 // min(n_classes, 16) + 1)[:16]\n",
    "\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    G.train()\n",
    "    D.train()\n",
    "    \n",
    "    # Metrics for this epoch\n",
    "    epoch_d_loss = 0.0\n",
    "    epoch_g_loss = 0.0\n",
    "    epoch_d_real_accuracy = 0.0\n",
    "    epoch_d_fake_accuracy = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for i, (real_images, real_labels) in progress_bar:\n",
    "        # Move data to device\n",
    "        real_images = real_images.to(device)\n",
    "        real_labels = real_labels.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        # Create labels for real and fake images\n",
    "        real_target = torch.ones(batch_size, 1, device=device)\n",
    "        fake_target = torch.zeros(batch_size, 1, device=device)\n",
    "        \n",
    "        # ---------------------\n",
    "        # Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Forward pass real batch through D\n",
    "        d_real_output = D(real_images, real_labels)\n",
    "        d_real_loss = criterion(d_real_output, real_target)\n",
    "        \n",
    "        # Generate batch of fake images\n",
    "        z = torch.randn(batch_size, noise_dim, device=device)\n",
    "        \n",
    "        # Use weighted sampling for fake labels to address class imbalance\n",
    "        fake_labels_dist = torch.multinomial(class_weights, batch_size, replacement=True)\n",
    "        \n",
    "        fake_images = G(z, fake_labels_dist)\n",
    "        \n",
    "        # Classify fake batch with D\n",
    "        d_fake_output = D(fake_images.detach(), fake_labels_dist)\n",
    "        d_fake_loss = criterion(d_fake_output, fake_target)\n",
    "        \n",
    "        # Add losses\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        \n",
    "        # Calculate metrics\n",
    "        d_real_accuracy = ((d_real_output > 0).float().mean()).item()\n",
    "        d_fake_accuracy = ((d_fake_output < 0).float().mean()).item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # ---------------------\n",
    "        # Train Generator\n",
    "        # ---------------------\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Generate new batch of fake images\n",
    "        # We reuse z and fake_labels_dist from above\n",
    "        fake_images = G(z, fake_labels_dist)\n",
    "        \n",
    "        # Try to fool the discriminator\n",
    "        g_output = D(fake_images, fake_labels_dist)\n",
    "        g_loss = criterion(g_output, real_target)  # We want generator to produce images discriminator thinks are real\n",
    "        \n",
    "        # Backpropagation\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        epoch_d_loss += d_loss.item()\n",
    "        epoch_g_loss += g_loss.item()\n",
    "        epoch_d_real_accuracy += d_real_accuracy\n",
    "        epoch_d_fake_accuracy += d_fake_accuracy\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'D Loss': f\"{d_loss.item():.4f}\",\n",
    "            'G Loss': f\"{g_loss.item():.4f}\",\n",
    "            'D Real Acc': f\"{d_real_accuracy:.3f}\",\n",
    "            'D Fake Acc': f\"{d_fake_accuracy:.3f}\"\n",
    "        })\n",
    "    \n",
    "    # End of epoch\n",
    "    avg_d_loss = epoch_d_loss / len(data_loader)\n",
    "    avg_g_loss = epoch_g_loss / len(data_loader)\n",
    "    avg_d_real_acc = epoch_d_real_accuracy / len(data_loader)\n",
    "    avg_d_fake_acc = epoch_d_fake_accuracy / len(data_loader)\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalar('Loss/Discriminator', avg_d_loss, epoch)\n",
    "    writer.add_scalar('Loss/Generator', avg_g_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Discriminator_Real', avg_d_real_acc, epoch)\n",
    "    writer.add_scalar('Accuracy/Discriminator_Fake', avg_d_fake_acc, epoch)\n",
    "    \n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}] - \"\n",
    "          f\"D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}, \"\n",
    "          f\"D Real Acc: {avg_d_real_acc:.3f}, D Fake Acc: {avg_d_fake_acc:.3f}\")\n",
    "    \n",
    "    # Generate and log sample images\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        fake_samples, sample_labels = generate_samples(num_samples=16, fixed_noise=fixed_noise, fixed_labels=fixed_labels)\n",
    "        fig = create_image_grid(fake_samples, sample_labels)\n",
    "        \n",
    "        # Save figure to TensorBoard\n",
    "        writer.add_figure(f'Generated Traffic Signs/Epoch {epoch+1}', fig, epoch)\n",
    "        \n",
    "        # Save figure to disk\n",
    "        os.makedirs('generated_samples', exist_ok=True)\n",
    "        fig.savefig(f'generated_samples/epoch_{epoch+1}.png')\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "        save_checkpoint(G, D, optimizer_G, optimizer_D, epoch, f'checkpoints/gan_checkpoint_epoch_{epoch+1}.pth')\n",
    "\n",
    "# Save final model\n",
    "save_checkpoint(G, D, optimizer_G, optimizer_D, num_epochs, 'checkpoints/gan_final_model.pth')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate final samples\n",
    "fake_samples, sample_labels = generate_samples(num_samples=36)\n",
    "fig = create_image_grid(fake_samples, sample_labels, nrow=6)\n",
    "fig.savefig('final_generated_samples.png')\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
